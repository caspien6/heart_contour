{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "from roi import RoiLearn\n",
    "from roi_dataset import RoiDataset\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "import preprocess_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_inp, n_hidden):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(n_inp, n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_inp)\n",
    "        self.n_inp = n_inp\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.sigmoid(self.encoder(x))\n",
    "        decoded = F.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    '''\n",
    "    args:\n",
    "        2 tensors `p` and `q`\n",
    "    returns:\n",
    "        kl divergence between the softmax of `p` and `q`\n",
    "    '''\n",
    "    p = F.softmax(p)\n",
    "    q = F.softmax(q)\n",
    "\n",
    "    s1 = torch.sum(p * torch.log(p / q))\n",
    "    s2 = torch.sum((1 - p) * torch.log((1 - p) / (1 - q)))\n",
    "    return s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class RoiLearn:\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(12)\n",
    "        self.conv1 = nn.Conv2d(1,100, (11,11))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avgpool = nn.AvgPool2d(6)\n",
    "        self.flatten = Flatten()\n",
    "        self.full = nn.Linear(8100,1024)\n",
    "        #self.encoder = nn.Linear(121,100)\n",
    "        #self.decoder = nn.Linear(100,121)\n",
    "          \n",
    "    # Autoencoder architecture\n",
    "    def build_ae(self):\n",
    "        self.autoencoder = Autoencoder(121,100)\n",
    "        self.autoencoder = self.autoencoder.double()\n",
    "        \n",
    "    # Autoencoder W2 and b2 to the original model conv1 layer features and biases.\n",
    "    # From the parameters list - index 0 is the weights\n",
    "    #                          - index 1 is the biases\n",
    "    def ae_weights2model_feature_set(self):\n",
    "        \n",
    "        w2 = list(self.encoder.parameters())\n",
    "        \n",
    "        b2 = w2[1].detach().numpy()\n",
    "        # weights shape here (100,121)\n",
    "        w2 = np.expand_dims(w2[0].detach().numpy().reshape((100,11,11)), axis = 1)\n",
    "        # weights shape (100,1,11,11)\n",
    "        \n",
    "        conv1_features = list(self.conv1.parameters())\n",
    "        conv1_features[0] = torch.nn.Parameter(torch.from_numpy(w2))\n",
    "        conv1_features[1] = torch.nn.Parameter(torch.from_numpy(b2))\n",
    "        conv1_features[0].requires_grad=False\n",
    "        conv1_features[1].requires_grad=False\n",
    "        \n",
    "    # Rho - sparsity penalty pj = p    \n",
    "    def learn_ae(self, dataset_loader, optimizer, ep = 1, lr = 0.01, rho = torch.tensor(0.1).double(),BETA = 3, RHO = 0.1):\n",
    "        rho = torch.FloatTensor([RHO for _ in range(self.autoencoder.n_hidden)]).unsqueeze(0).double()\n",
    "        for epoch in range(ep):\n",
    "            for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "                #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "                #print(sample_batched['image'].shape)\n",
    "                \n",
    "                encoded, decoded = self.autoencoder(sample_batched['image'])\n",
    "                MSE_loss = (sample_batched['image'] - decoded) ** 2\n",
    "                MSE_loss.view(1, -1).sum(1)\n",
    "                MSE_loss = MSE_loss.view(1, -1).sum(1) / dataset_loader.batch_size\n",
    "                \n",
    "                #y_pred = self.autoencoder(sample_batched['image'])\n",
    "                rho_hat = torch.sum(encoded, dim=0, keepdim=True)\n",
    "                sparsity_penalty = BETA * kl_divergence(rho, rho_hat)\n",
    "                \n",
    "                loss = MSE_loss + sparsity_penalty\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = nn.Sequential(self.conv1,\n",
    "                            self.avgpool,\n",
    "                            self.softmax,\n",
    "                            self.flatten,\n",
    "                            self.full,\n",
    "                            self.softmax\n",
    "                            )\n",
    "        self.model = self.model.double()\n",
    "    \n",
    "    def propagate_from_dataLoader(self,dl):\n",
    "        for i_batch, sample_batched in enumerate(dl):\n",
    "            print(self.model(sample_batched[0]))\n",
    "        \n",
    "    def propagate(self):\n",
    "        return self.model(self.x)\n",
    "    \n",
    "    def save_image( self,npdata, outfilename ) :\n",
    "        img = Image.fromarray( np.asarray( np.clip(npdata,0,255), dtype=\"uint8\"), \"L\" )\n",
    "        img.save( outfilename )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.1)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=32, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.057048788414086345\n",
      "epoch:  0  loss:  0.04729243769189851\n",
      "epoch:  0  loss:  0.04293113599102212\n",
      "epoch:  0  loss:  0.08400067271933016\n",
      "epoch:  1  loss:  0.04485048988890402\n",
      "epoch:  1  loss:  0.04549884135343655\n",
      "epoch:  1  loss:  0.055224178017018226\n",
      "epoch:  1  loss:  0.04318030769345006\n",
      "epoch:  2  loss:  0.045308962202390855\n",
      "epoch:  2  loss:  0.05198700108739078\n",
      "epoch:  2  loss:  0.048659241390528456\n",
      "epoch:  2  loss:  0.0471678105157725\n",
      "epoch:  3  loss:  0.051382407001028065\n",
      "epoch:  3  loss:  0.05440652940344873\n",
      "epoch:  3  loss:  0.04237277964071936\n",
      "epoch:  3  loss:  0.020554168352146192\n",
      "epoch:  4  loss:  0.04902342385171187\n",
      "epoch:  4  loss:  0.04908311544058612\n",
      "epoch:  4  loss:  0.04788624531611998\n",
      "epoch:  4  loss:  0.055437640439704276\n",
      "epoch:  5  loss:  0.04632165488682708\n",
      "epoch:  5  loss:  0.046484143068982915\n",
      "epoch:  5  loss:  0.0543376864165634\n",
      "epoch:  5  loss:  0.037882382192694866\n",
      "epoch:  6  loss:  0.04705513791360042\n",
      "epoch:  6  loss:  0.043515008827554956\n",
      "epoch:  6  loss:  0.05508711387853865\n",
      "epoch:  6  loss:  0.06493783999549363\n",
      "epoch:  7  loss:  0.0477549091421766\n",
      "epoch:  7  loss:  0.05151039647585263\n",
      "epoch:  7  loss:  0.04704913065923091\n",
      "epoch:  7  loss:  0.057773642129065585\n",
      "epoch:  8  loss:  0.049302878574640975\n",
      "epoch:  8  loss:  0.04991235812999693\n",
      "epoch:  8  loss:  0.04678477248514692\n",
      "epoch:  8  loss:  0.06591217449052209\n",
      "epoch:  9  loss:  0.03957455126178049\n",
      "epoch:  9  loss:  0.043511190461797654\n",
      "epoch:  9  loss:  0.0632162586058343\n",
      "epoch:  9  loss:  0.060876982216415264\n"
     ]
    }
   ],
   "source": [
    "# if we don't have the .csv file\n",
    "preprocess_img.write_all_rectangle2file('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0603185b58ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# Forward Propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Compute and print loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mO:\\ProgrammingSoftwares\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mO:\\ProgrammingSoftwares\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mO:\\ProgrammingSoftwares\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mO:\\ProgrammingSoftwares\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for epoch in range(100):\n",
    "    for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "        #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "\n",
    "        # Forward Propagation\n",
    "        y_pred = roi.model(sample_batched['image'])\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, sample_batched['mask'])\n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "                # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "            #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "\n",
    "            # Forward Propagation\n",
    "    y_pred = roi.model(sample_batched['image'])\n",
    "            \n",
    "    y_pred = np.reshape(y_pred.detach().numpy()[0], (32,32))\n",
    "    img = Image.fromarray(y_pred, 'L')\n",
    "    img.show()\n",
    "    if i_batch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "from roi_dataset import RoiDataset\n",
    "import transform_classes\n",
    "from roi import RoiLearn\n",
    "import torch\n",
    "\n",
    "dcm_folder = \"O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\17108976AMR804\\\\1001\\\\imgs\\\\\"\n",
    "con_file = \"O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\17108976AMR804\\\\1001\\\\contour.con\"\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "\n",
    "\n",
    "write_rectangle2file(dcm_folder, con_file, csv_file)\n",
    "\n",
    "\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, dcm_folder, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.001)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=8, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_changed = False\n",
    "for epoch in range(100):\n",
    "    for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "        #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "\n",
    "        # Forward Propagation\n",
    "        y_pred = roi.model(sample_batched['image'])\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, sample_batched['mask'])\n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "                # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "            #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "\n",
    "            # Forward Propagation\n",
    "    y_pred = roi.model(sample_batched['image'])\n",
    "            \n",
    "    y_pred = np.reshape(y_pred.detach().numpy()[0], (32,32))\n",
    "    img = Image.fromarray(y_pred, 'L')\n",
    "    img.show()\n",
    "    if i_batch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.1)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=32, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n"
     ]
    }
   ],
   "source": [
    "# if we don't have the .csv file\n",
    "preprocess_img.write_all_rectangle2file('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  135.04837830955535\n",
      "epoch:  1  loss:  128.15998807795094\n",
      "epoch:  2  loss:  110.37061060556725\n",
      "epoch:  3  loss:  122.25503991519653\n",
      "epoch:  4  loss:  117.5149186699118\n",
      "epoch:  5  loss:  119.85674924642241\n",
      "epoch:  6  loss:  118.53525961351876\n",
      "epoch:  7  loss:  116.07255359897056\n",
      "epoch:  8  loss:  118.26996631444287\n",
      "epoch:  9  loss:  118.46232447794543\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "\n",
    "compose3 = Compose([transform_classes.GetRandomPatch(),transform_classes.StandardScale2(),transform_classes.ToTensor()])\n",
    "\n",
    "ds2 = RoiDataset(csv_file, compose3)\n",
    "roi = RoiLearn()\n",
    "roi.build_ae()\n",
    "\n",
    "crit = torch.nn.MSELoss()\n",
    "opt = torch.optim.SGD(roi.autoencoder.parameters(), lr=0.001, weight_decay = 0.0001 )\n",
    "\n",
    "# Random 1000 sample\n",
    "weighted_rnd_sample = torch.utils.data.WeightedRandomSampler([float(1/len(ds2)) for i in range(len(ds2))], 1000, replacement=True)\n",
    "dataset_loader = torch.utils.data.DataLoader(ds2,batch_size=8, num_workers=0, sampler=weighted_rnd_sample)\n",
    "\n",
    "roi.learn_ae(dataset_loader, optimizer = opt,  ep = 10)\n",
    "\n",
    "#roi.ae_weights2model_feature_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.1)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=32, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
