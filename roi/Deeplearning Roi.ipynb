{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "from roi import RoiLearn\n",
    "from roi_dataset import RoiDataset\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "import preprocess_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we don't have the .csv file\n",
    "preprocess_img.write_all_rectangle2file('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch.nn.functional as F\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class RoiLearn:\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(12)\n",
    "        self.conv1 = nn.Conv2d(1,100, (11,11))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avgpool = nn.AvgPool2d(6)\n",
    "        self.flatten = Flatten()\n",
    "        self.full = nn.Linear(8100,1024)\n",
    "        #self.encoder = nn.Linear(121,100)\n",
    "        #self.decoder = nn.Linear(100,121)\n",
    "          \n",
    "    # Autoencoder architecture\n",
    "    def build_ae(self):\n",
    "        self.autoencoder = Autoencoder(121,100)\n",
    "        self.autoencoder = self.autoencoder.double()\n",
    "        \n",
    "    # Autoencoder W2 and b2 to the original model conv1 layer features and biases.\n",
    "    # From the parameters list - index 0 is the weights\n",
    "    #                          - index 1 is the biases\n",
    "    def ae_weights2model_feature_set(self):\n",
    "        \n",
    "        w2 = list(self.encoder.parameters())\n",
    "        \n",
    "        b2 = w2[1].detach().numpy()\n",
    "        # weights shape here (100,121)\n",
    "        w2 = np.expand_dims(w2[0].detach().numpy().reshape((100,11,11)), axis = 1)\n",
    "        # weights shape (100,1,11,11)\n",
    "        \n",
    "        conv1_features = list(self.conv1.parameters())\n",
    "        conv1_features[0] = torch.nn.Parameter(torch.from_numpy(w2))\n",
    "        conv1_features[1] = torch.nn.Parameter(torch.from_numpy(b2))\n",
    "        conv1_features[0].requires_grad=False\n",
    "        conv1_features[1].requires_grad=False\n",
    "        \n",
    "    \n",
    "    def normalize_range(self, vector):\n",
    "        min_v = torch.min(vector)        \n",
    "        range_v = torch.max(vector) - min_v\n",
    "        \n",
    "        if range_v > 0:\n",
    "            normalised = (vector - min_v) / range_v\n",
    "        else:\n",
    "            normalised = torch.zeros(vector.size())\n",
    "        return normalised\n",
    "\n",
    "    ''' Learn the autoencoder features for the original convolution weights.\n",
    "        Params:\n",
    "            dataset_loader - the prepared dataset inside a configured pytorch dataloader\n",
    "            optimizer - for the backpropagation\n",
    "            criterion - method for the half part of the loss function\n",
    "            ep - epochs\n",
    "            lr - learning rate\n",
    "            BETA - weightening the sparsity part of the loss function\n",
    "            RHO - the sample distribution for comparing the average activation (sparse part of the loss function too.)\n",
    "    '''     \n",
    "    def learn_ae(self, dataset_loader, optimizer,criterion, ep = 1, lr = 0.01, BETA = 3, RHO = 0.1):\n",
    "        \n",
    "        rho = torch.tensor([RHO for _ in range(self.autoencoder.n_hidden)]).double()\n",
    "        crit2 = nn.KLDivLoss(size_average=False)\n",
    "        for epoch in range(ep):\n",
    "            for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "    \n",
    "                # Forward\n",
    "                encoded, decoded = self.autoencoder(sample_batched['image'])\n",
    "                # Loss\n",
    "                # first loss is the loss what the user can choose\n",
    "                first_loss = criterion(self.normalize_range(sample_batched['image']), decoded)                \n",
    "                # the second loss member is the penalty loss, this helps the higher feature learning\n",
    "                sparsity_loss = crit2( F.log_softmax(torch.mean(encoded, dim = 0)) , rho)        \n",
    "                loss = first_loss + BETA*sparsity_loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "        \n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = nn.Sequential(self.conv1,\n",
    "                            self.avgpool,\n",
    "                            self.softmax,\n",
    "                            self.flatten,\n",
    "                            self.full,\n",
    "                            self.softmax\n",
    "                            )\n",
    "        self.model = self.model.double()\n",
    "    \n",
    "    def propagate_from_dataLoader(self,dl):\n",
    "        for i_batch, sample_batched in enumerate(dl):\n",
    "            print(self.model(sample_batched[0]))\n",
    "        \n",
    "    def propagate(self):\n",
    "        return self.model(self.x)\n",
    "    \n",
    "    def save_image( self,npdata, outfilename ) :\n",
    "        img = Image.fromarray( np.asarray( np.clip(npdata,0,255), dtype=\"uint8\"), \"L\" )\n",
    "        img.save( outfilename )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  74.51790440643512\n",
      "epoch:  1  loss:  79.44945902986198\n",
      "epoch:  2  loss:  74.51627841082727\n",
      "epoch:  3  loss:  114.10843521550902\n",
      "epoch:  4  loss:  82.39478955738822\n",
      "epoch:  5  loss:  77.0090664165455\n",
      "epoch:  6  loss:  109.3406306052832\n",
      "epoch:  7  loss:  75.65838149029878\n",
      "epoch:  8  loss:  74.25893228328295\n",
      "epoch:  9  loss:  93.3778252104612\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "\n",
    "compose3 = Compose([transform_classes.GetRandomPatch(),transform_classes.StandardScale2(),transform_classes.ToTensor()])\n",
    "\n",
    "ds2 = RoiDataset(csv_file, compose3)\n",
    "roi = RoiLearn()\n",
    "roi.build_ae()\n",
    "\n",
    "crit = torch.nn.MSELoss(size_average = True)\n",
    "opt = torch.optim.Adam(roi.autoencoder.parameters(),  weight_decay = 0.0001 )\n",
    "\n",
    "# Random 1000 sample\n",
    "weighted_rnd_sample = torch.utils.data.WeightedRandomSampler([float(1/len(ds2)) for i in range(len(ds2))], 1000, replacement=True)\n",
    "dataset_loader = torch.utils.data.DataLoader(ds2,batch_size=8, num_workers=0, sampler=weighted_rnd_sample)\n",
    "\n",
    "roi.learn_ae(dataset_loader, optimizer = opt, criterion = crit,  ep = 10)\n",
    "#roi.ae_weights2model_feature_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAAB5CAAAAAA4kdGVAAABjUlEQVR4nO3aTSuEURiH8bk1pWQklA9A8pIaFiyNBQsLH0GxVLJQSpSXtSgWKEKp2SmlFMo3ECKNwiwQJTuRDTvXvXh2Fuqc/7P6LZ45V92L05k5YxWp32cHtsFGuAvL4Ca8g13wAt7AktR/PSqrrLLKf3+sGy/AZfcGvIWfsB4ew0f4DadgjNNWWWWVwynbEHYnxyLcgnm4DZfgIXRnWrduK4xx2iqrrHI4ZUvjRTgHB+AJzEK3ObbDHJyHVzDGaausssrhlK0JN8Bq6DbHfei+mJ/BGtgMO2APjHHaKquscjhlyznDd1iEH7APrsORxI91Qvcra4zTVllllcMpp90h8hmewlFYBR/gBlyD4/AFuk03xmmrrLLK4ZStgGfhE8y4l2EvHEt8dw+6ayW3kcY4bZVVVjmcsvXjQiK/4DWcgZXwHh7AV1gKY5y2yiqrHE7ZyvE5XIFv0F0gDcNL6O7nj6D7k+c0jHHaKquscjhlG8Tu5FgHJ+AkXIVZWAtbEtd1i8U4bZVVVjmc8g9icUB81kHOKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=121x121 at 0x251D3467128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "#roi.autoencoder()\n",
    "idx = 4\n",
    "_,decoded = roi.autoencoder(ds2.__getitem__(idx)['image'])\n",
    "image = Image.fromarray( np.reshape(decoded.detach().numpy()*255, (11,11)), mode='L')\n",
    "image2 = Image.fromarray( np.reshape(ds2.__getitem__(idx)['image'].numpy()*255, (11,11)), mode='L')\n",
    "image.resize((121,121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAAB5CAAAAAA4kdGVAAABiUlEQVR4nO3bvyvEcRzH8fvwZTvFhDJYbrhsfmRQsgh1TK7rFqNYFYoyoJQY/RisBixGm/wDFgbKH8B0ZbBQNs/38KlL36Len9dnenTDPa/3cH3u8/1cmCz8rDG4CjvhJmyHd3AFbsMjeAVbCv+1VFZZZZXzr3CPa/ATTsF5WIWP0Xd4hW+wD6Y4bZVVVtlPOTvGp7AMu2Ab3IM9cBwOwwV4A1Octsoqq+ynnHXgCtyBH9D8tH+JvtoK++EonIYpTltllVX2Uw67uOlZ5jJcgrOwG5pzz154BlOctsoqq+ynHG7xOjyHc/AdNuAzNFvLIryA5hggxWmrrLLKfsrZBDZnmSVozjKHoDnLNHvPS2i+dEdgA6Y4bZVVVtlPOZgH4vGd4yF8gmtwC17DRXgCze41xWmrrLLKfsrBPF3Pf5Z5AJteaUpx2iqrrLKfchjA8XuZg7AOf3OlaQOaY4AUp62yyir7KWfxe5kP0Hy2GWge1Zv95Bc0/1YyV5r2o+/7x0tllVVWOf/6BifTNAnU+XHYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=121x121 at 0x251BB671CF8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2.resize((121,121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.1)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=32, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
