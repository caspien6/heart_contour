{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "from roi import RoiLearn\n",
    "from roi_dataset import RoiDataset\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "import preprocess_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_inp, n_hidden):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(n_inp, n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_inp)\n",
    "        self.n_inp = n_inp\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.sigmoid(self.encoder(x))\n",
    "        decoded = F.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    '''\n",
    "    args:\n",
    "        2 tensors `p` and `q`\n",
    "    returns:\n",
    "        kl divergence between the softmax of `p` and `q`\n",
    "    '''\n",
    "    p = F.softmax(p)\n",
    "    q = F.softmax(q)\n",
    "\n",
    "    s1 = torch.sum(p * torch.log(p / q))\n",
    "    s2 = torch.sum((1 - p) * torch.log((1 - p) / (1 - q)))\n",
    "    return s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import png\n",
    "import pydicom\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class RoiLearn:\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(12)\n",
    "        self.conv1 = nn.Conv2d(1,100, (11,11))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avgpool = nn.AvgPool2d(6)\n",
    "        self.flatten = Flatten()\n",
    "        self.full = nn.Linear(8100,1024)\n",
    "        #self.encoder = nn.Linear(121,100)\n",
    "        #self.decoder = nn.Linear(100,121)\n",
    "          \n",
    "    # Autoencoder architecture\n",
    "    def build_ae(self):\n",
    "        self.autoencoder = Autoencoder(121,100)\n",
    "        self.autoencoder = self.autoencoder.double()\n",
    "        \n",
    "    # Autoencoder W2 and b2 to the original model conv1 layer features and biases.\n",
    "    # From the parameters list - index 0 is the weights\n",
    "    #                          - index 1 is the biases\n",
    "    def ae_weights2model_feature_set(self):\n",
    "        \n",
    "        w2 = list(self.encoder.parameters())\n",
    "        \n",
    "        b2 = w2[1].detach().numpy()\n",
    "        # weights shape here (100,121)\n",
    "        w2 = np.expand_dims(w2[0].detach().numpy().reshape((100,11,11)), axis = 1)\n",
    "        # weights shape (100,1,11,11)\n",
    "        \n",
    "        conv1_features = list(self.conv1.parameters())\n",
    "        conv1_features[0] = torch.nn.Parameter(torch.from_numpy(w2))\n",
    "        conv1_features[1] = torch.nn.Parameter(torch.from_numpy(b2))\n",
    "        conv1_features[0].requires_grad=False\n",
    "        conv1_features[1].requires_grad=False\n",
    "        \n",
    "    # Rho - sparsity penalty pj = p    \n",
    "    def learn_ae(self, dataset_loader, optimizer, ep = 1, lr = 0.01, rho = torch.tensor(0.1).double(),BETA = 3, RHO = 0.1):\n",
    "        rho = torch.FloatTensor([RHO for _ in range(self.autoencoder.n_hidden)]).unsqueeze(0).double()\n",
    "        for epoch in range(ep):\n",
    "            for i_batch, sample_batched in enumerate(dataset_loader):\n",
    "                #print(i_batch, sample_batched['image'].size(),sample_batched['mask'].size())\n",
    "                #print(sample_batched['image'].shape)\n",
    "                \n",
    "                encoded, decoded = self.autoencoder(sample_batched['image'])\n",
    "                MSE_loss = (sample_batched['image'] - decoded) ** 2\n",
    "                MSE_loss.view(1, -1).sum(1)\n",
    "                MSE_loss = MSE_loss.view(1, -1).sum(1) / dataset_loader.batch_size\n",
    "                \n",
    "                #y_pred = self.autoencoder(sample_batched['image'])\n",
    "                rho_hat = torch.sum(encoded, dim=0, keepdim=True)\n",
    "                sparsity_penalty = BETA * kl_divergence(rho, rho_hat)\n",
    "                \n",
    "                loss = MSE_loss + sparsity_penalty\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "                \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = nn.Sequential(self.conv1,\n",
    "                            self.avgpool,\n",
    "                            self.softmax,\n",
    "                            self.flatten,\n",
    "                            self.full,\n",
    "                            self.softmax\n",
    "                            )\n",
    "        self.model = self.model.double()\n",
    "    \n",
    "    def propagate_from_dataLoader(self,dl):\n",
    "        for i_batch, sample_batched in enumerate(dl):\n",
    "            print(self.model(sample_batched[0]))\n",
    "        \n",
    "    def propagate(self):\n",
    "        return self.model(self.x)\n",
    "    \n",
    "    def save_image( self,npdata, outfilename ) :\n",
    "        img = Image.fromarray( np.asarray( np.clip(npdata,0,255), dtype=\"uint8\"), \"L\" )\n",
    "        img.save( outfilename )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n",
      "Dicom files were read in!\n",
      "Con files were read in!\n"
     ]
    }
   ],
   "source": [
    "# if we don't have the .csv file\n",
    "preprocess_img.write_all_rectangle2file('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\heart_contour\\\\sa_all_1\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  135.04837830955535\n",
      "epoch:  1  loss:  128.15998807795094\n",
      "epoch:  2  loss:  110.37061060556725\n",
      "epoch:  3  loss:  122.25503991519653\n",
      "epoch:  4  loss:  117.5149186699118\n",
      "epoch:  5  loss:  119.85674924642241\n",
      "epoch:  6  loss:  118.53525961351876\n",
      "epoch:  7  loss:  116.07255359897056\n",
      "epoch:  8  loss:  118.26996631444287\n",
      "epoch:  9  loss:  118.46232447794543\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "\n",
    "compose3 = Compose([transform_classes.GetRandomPatch(),transform_classes.StandardScale2(),transform_classes.ToTensor()])\n",
    "\n",
    "ds2 = RoiDataset(csv_file, compose3)\n",
    "roi = RoiLearn()\n",
    "roi.build_ae()\n",
    "\n",
    "crit = torch.nn.MSELoss()\n",
    "opt = torch.optim.SGD(roi.autoencoder.parameters(), lr=0.001, weight_decay = 0.0001 )\n",
    "\n",
    "# Random 1000 sample\n",
    "weighted_rnd_sample = torch.utils.data.WeightedRandomSampler([float(1/len(ds2)) for i in range(len(ds2))], 1000, replacement=True)\n",
    "dataset_loader = torch.utils.data.DataLoader(ds2,batch_size=8, num_workers=0, sampler=weighted_rnd_sample)\n",
    "\n",
    "roi.learn_ae(dataset_loader, optimizer = opt,  ep = 10)\n",
    "\n",
    "#roi.ae_weights2model_feature_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.transforms import Compose\n",
    "import transform_classes\n",
    "\n",
    "csv_file = 'O:/ProgrammingSoftwares/anaconda_projects/heart_contour/sa_all_1/rectangle.csv'\n",
    "compose1 = Compose([transform_classes.ReScale64(),transform_classes.StandardScale(),transform_classes.ToTensor()])\n",
    "compose2 = Compose([transform_classes.ReScale32(),transform_classes.ToTensor()])\n",
    "ds = RoiDataset(csv_file, compose1, compose2)\n",
    "\n",
    "roi = RoiLearn()\n",
    "roi.build_model()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(roi.model.parameters(), lr=0.1)\n",
    "\n",
    "dataset_loader = torch.utils.data.DataLoader(ds,batch_size=32, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
